# -*- coding: utf-8 -*-
"""7 questions LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Frymy-uBmJtwZp2Bprgj3zNbUY5RwtIM

1. Implement Scaled Dot-Product Attention
Write code for Q, K, V computation.
Visualize attention weights.
"""

# Complete implementation of Scaled Dot-Product Attention
# Includes Q, K, V computation and visualization of attention weights

import numpy as np
import matplotlib.pyplot as plt

# -----------------------------
# Step 1: Setup
# -----------------------------
np.random.seed(0)

seq_len = 5      # number of tokens
d_model = 8      # embedding dimension
d_k = d_model    # key/query dimension

# -----------------------------
# Step 2: Input embeddings
# -----------------------------
X = np.random.randn(seq_len, d_model)

# -----------------------------
# Step 3: Weight matrices
# -----------------------------
W_Q = np.random.randn(d_model, d_k)
W_K = np.random.randn(d_model, d_k)
W_V = np.random.randn(d_model, d_k)

# -----------------------------
# Step 4: Compute Q, K, V
# -----------------------------
Q = X @ W_Q
K = X @ W_K
V = X @ W_V

# -----------------------------
# Step 5: Scaled Dot-Product Attention
# -----------------------------
scores = (Q @ K.T) / np.sqrt(d_k)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

attention_weights = softmax(scores)
output = attention_weights @ V

# -----------------------------
# Step 6: Visualization
# -----------------------------
plt.figure()
plt.imshow(attention_weights)
plt.colorbar()
plt.xlabel("Key Positions")
plt.ylabel("Query Positions")
plt.title("Scaled Dot-Product Attention Weights")
plt.show()

# -----------------------------
# Step 7: Print results
# -----------------------------
print("Q matrix:\n", Q)
print("\nK matrix:\n", K)
print("\nV matrix:\n", V)
print("\nAttention Weights:\n", attention_weights)
print("\nAttention Output:\n", output)

"""2. Build Self-Attention from Scratch
Implement a single-head self-attention layer in PyTorch/TensorFlow.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# -------------------------------
# Single-Head Self-Attention Layer
# -------------------------------
class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super(SelfAttention, self).__init__()

        self.d_model = d_model

        # Learnable projection matrices
        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)

    def forward(self, X):
        """
        X shape: (batch_size, seq_len, d_model)
        """
        # Step 1: Compute Q, K, V
        Q = self.W_Q(X)
        K = self.W_K(X)
        V = self.W_V(X)

        # Step 2: Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)

        # Step 3: Softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)

        # Step 4: Weighted sum of values
        output = torch.matmul(attention_weights, V)

        return output, attention_weights


# -------------------------------
# Example Usage
# -------------------------------
if __name__ == "__main__":
    torch.manual_seed(0)

    batch_size = 2
    seq_len = 5
    d_model = 8

    # Dummy input embeddings
    X = torch.randn(batch_size, seq_len, d_model)

    # Initialize self-attention layer
    self_attention = SelfAttention(d_model)

    # Forward pass
    output, attn_weights = self_attention(X)

    print("Input shape:", X.shape)
    print("Output shape:", output.shape)
    print("Attention weights shape:", attn_weights.shape)

"""3. Implement Multi-Head Attention
Extend single-head attention to multi-head.
Compare outputs across heads.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# --------------------------------
# Multi-Head Self-Attention
# --------------------------------
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()

        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_head = d_model // num_heads

        # Linear projections
        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)

        # Output projection
        self.W_O = nn.Linear(d_model, d_model, bias=False)

    def split_heads(self, x):
        """
        x shape: (batch_size, seq_len, d_model)
        return: (batch_size, num_heads, seq_len, d_head)
        """
        batch_size, seq_len, _ = x.size()
        x = x.view(batch_size, seq_len, self.num_heads, self.d_head)
        return x.transpose(1, 2)

    def forward(self, X):
        batch_size, seq_len, _ = X.size()

        # Step 1: Linear projections
        Q = self.W_Q(X)
        K = self.W_K(X)
        V = self.W_V(X)

        # Step 2: Split into multiple heads
        Q = self.split_heads(Q)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # Step 3: Scaled dot-product attention (per head)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        attention_weights = F.softmax(scores, dim=-1)
        head_outputs = torch.matmul(attention_weights, V)

        # Step 4: Concatenate heads
        head_outputs = head_outputs.transpose(1, 2)
        concat = head_outputs.contiguous().view(batch_size, seq_len, self.d_model)

        # Step 5: Final linear projection
        output = self.W_O(concat)

        return output, attention_weights, head_outputs


# --------------------------------
# Example + Head Comparison
# --------------------------------
if __name__ == "__main__":
    torch.manual_seed(0)

    batch_size = 1
    seq_len = 5
    d_model = 8
    num_heads = 2

    X = torch.randn(batch_size, seq_len, d_model)

    mha = MultiHeadAttention(d_model, num_heads)

    output, attn_weights, head_outputs = mha(X)

    print("Final Output Shape:", output.shape)
    print("Attention Weights Shape:", attn_weights.shape)
    print("Per-Head Output Shape:", head_outputs.shape)

    # Compare head outputs
    print("\nHead 0 Output:\n", head_outputs[0, 0])
    print("\nHead 1 Output:\n", head_outputs[0, 1])

"""4. Attention Visualization
Use BERTViz or custom code to visualize attention maps.
Interpret which tokens influence predictions.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import matplotlib.pyplot as plt

# -------------------------------
# Simple Multi-Head Attention
# -------------------------------
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0

        self.num_heads = num_heads
        self.d_head = d_model // num_heads

        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)

    def forward(self, X):
        B, T, D = X.shape

        Q = self.W_Q(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)
        K = self.W_K(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)
        V = self.W_V(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        attn = F.softmax(scores, dim=-1)

        return attn  # (batch, heads, seq_len, seq_len)


# -------------------------------
# Example sentence & tokens
# -------------------------------
tokens = ["I", "love", "deep", "learning"]
seq_len = len(tokens)

torch.manual_seed(0)
X = torch.randn(1, seq_len, 8)

mha = MultiHeadAttention(d_model=8, num_heads=2)
attention_weights = mha(X)

# -------------------------------
# Visualize attention for head 0
# -------------------------------
head = 0
attn_matrix = attention_weights[0, head].detach().numpy()

plt.figure()
plt.imshow(attn_matrix)
plt.colorbar()
plt.xticks(range(seq_len), tokens)
plt.yticks(range(seq_len), tokens)
plt.xlabel("Key Tokens")
plt.ylabel("Query Tokens")
plt.title(f"Attention Map (Head {head})")
plt.show()

"""6. Transformer Encoder Block
Implement a full encoder block (Attention + FFN + LayerNorm).

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# --------------------------------
# Multi-Head Self-Attention
# --------------------------------
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0

        self.num_heads = num_heads
        self.d_head = d_model // num_heads

        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)
        self.W_O = nn.Linear(d_model, d_model, bias=False)

    def forward(self, X):
        B, T, D = X.shape

        Q = self.W_Q(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)
        K = self.W_K(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)
        V = self.W_V(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        attn = F.softmax(scores, dim=-1)

        head_outputs = torch.matmul(attn, V)

        head_outputs = head_outputs.transpose(1, 2).contiguous()
        concat = head_outputs.view(B, T, D)

        return self.W_O(concat)


# --------------------------------
# Position-wise Feed Forward Network
# --------------------------------
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))


# --------------------------------
# Transformer Encoder Block
# --------------------------------
class TransformerEncoderBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()

        self.attention = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForward(d_model, d_ff)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, X):
        # 1Ô∏è‚É£ Self-Attention + Add & Norm
        attn_output = self.attention(X)
        X = self.norm1(X + self.dropout(attn_output))

        # 2Ô∏è‚É£ Feed Forward + Add & Norm
        ffn_output = self.ffn(X)
        X = self.norm2(X + self.dropout(ffn_output))

        return X


# --------------------------------
# Example Usage
# --------------------------------
if __name__ == "__main__":
    torch.manual_seed(0)

    batch_size = 2
    seq_len = 5
    d_model = 8
    num_heads = 2
    d_ff = 32

    X = torch.randn(batch_size, seq_len, d_model)

    encoder_block = TransformerEncoderBlock(
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff
    )

    output = encoder_block(X)

    print("Input shape :", X.shape)
    print("Output shape:", output.shape)

"""5. Head Importance Analysis
Remove one attention head at a time.
Measure impact on accuracy.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# -------------------------------
# Multi-Head Attention (with masking)
# -------------------------------
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0

        self.num_heads = num_heads
        self.d_head = d_model // num_heads

        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)
        self.W_O = nn.Linear(d_model, d_model, bias=False)

    def forward(self, X, head_mask=None):
        B, T, D = X.shape

        Q = self.W_Q(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)
        K = self.W_K(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)
        V = self.W_V(X).view(B, T, self.num_heads, self.d_head).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)
        attn = F.softmax(scores, dim=-1)

        head_outputs = torch.matmul(attn, V)

        # üî¥ Head ablation
        if head_mask is not None:
            head_outputs = head_outputs * head_mask.view(1, -1, 1, 1)

        head_outputs = head_outputs.transpose(1, 2).contiguous()
        concat = head_outputs.view(B, T, D)

        return self.W_O(concat)


# -------------------------------
# Simple Transformer Classifier
# -------------------------------
class TransformerClassifier(nn.Module):
    def __init__(self, d_model, num_heads, num_classes):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(d_model, num_classes)

    def forward(self, X, head_mask=None):
        X = self.attn(X, head_mask)
        X = X.transpose(1, 2)
        X = self.pool(X).squeeze(-1)
        return self.fc(X)


# -------------------------------
# Accuracy Evaluation
# -------------------------------
def evaluate_accuracy(model, X, y, head_mask=None):
    model.eval()
    with torch.no_grad():
        logits = model(X, head_mask)
        preds = logits.argmax(dim=1)
        acc = (preds == y).float().mean().item()
    return acc


# -------------------------------
# Head Importance Analysis
# -------------------------------
def head_importance_analysis(model, X, y):
    num_heads = model.attn.num_heads

    baseline_acc = evaluate_accuracy(model, X, y)
    print(f"\nBaseline Accuracy: {baseline_acc:.4f}\n")

    print("Accuracy Drop Per Head:")
    for h in range(num_heads):
        head_mask = torch.ones(num_heads)
        head_mask[h] = 0.0  # remove head h

        acc = evaluate_accuracy(model, X, y, head_mask)
        drop = baseline_acc - acc
        print(f"Head {h}: Accuracy Drop = {drop:.4f}")


# -------------------------------
# Run Everything
# -------------------------------
if __name__ == "__main__":
    torch.manual_seed(0)

    # Dummy dataset
    batch_size = 128
    seq_len = 10
    d_model = 16
    num_heads = 4
    num_classes = 2

    X = torch.randn(batch_size, seq_len, d_model)
    y = torch.randint(0, num_classes, (batch_size,))

    model = TransformerClassifier(d_model, num_heads, num_classes)

    head_importance_analysis(model, X, y)

"""7. Positional Encoding Experiment
Implement sinusoidal positional encoding.
Compare with learned positional embeddings.

"""

import torch
import torch.nn as nn
import math

# -------------------------------
# Sinusoidal Positional Encoding
# -------------------------------
class SinusoidalPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)

        div_term = torch.exp(
            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)
        )

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer("pe", pe)

    def forward(self, x):
        """
        x: (batch_size, seq_len, d_model)
        """
        return x + self.pe[:, :x.size(1)]


# -------------------------------
# Learned Positional Encoding
# -------------------------------
class LearnedPositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super().__init__()
        self.pos_embedding = nn.Embedding(max_len, d_model)

    def forward(self, x):
        """
        x: (batch_size, seq_len, d_model)
        """
        batch_size, seq_len, _ = x.size()
        positions = torch.arange(seq_len, device=x.device)
        positions = positions.unsqueeze(0).expand(batch_size, seq_len)
        return x + self.pos_embedding(positions)


# -------------------------------
# Experiment
# -------------------------------
if __name__ == "__main__":
    torch.manual_seed(0)

    batch_size = 2
    seq_len = 6
    d_model = 8

    # Dummy token embeddings (no position info)
    X = torch.zeros(batch_size, seq_len, d_model)

    # Positional encoders
    sinusoidal_pe = SinusoidalPositionalEncoding(d_model)
    learned_pe = LearnedPositionalEncoding(d_model)

    # Apply encodings
    X_sinusoidal = sinusoidal_pe(X)
    X_learned = learned_pe(X)

    print("Sinusoidal Positional Encoding (token 0):")
    print(X_sinusoidal[0, 0])

    print("\nLearned Positional Encoding (token 0):")
    print(X_learned[0, 0])

    print("\nSinusoidal Encoding Matrix:")
    print(X_sinusoidal[0])

    print("\nLearned Encoding Matrix:")
    print(X_learned[0])