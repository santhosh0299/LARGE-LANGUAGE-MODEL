# -*- coding: utf-8 -*-
"""P1 TOKENIZATION(6).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Chxg0lGuCgT2ltxVkXS_mq8_YnXpvZW
"""

from transformers import AutoTokenizer

# Load tokenizer of an LLM (example: GPT-2)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Input text
text = "He is from Alliance University"

# Tokenize text
tokens = tokenizer(text)

# Print tokenized output
print(tokens)

# Convert tokens to token IDs
token_ids = tokenizer.encode(text)
print("Token IDs:", token_ids)

# Convert token IDs back to text
decoded_text = tokenizer.decode(token_ids)
print("Decoded Text:", decoded_text)

# Whitespace tokenization example

text = "I am going to playground"

# Tokenize using whitespace
tokens = text.split()

print("Tokens:", tokens)

# Character-level tokenization example

text = "Bangalore city"

# Convert text to characters (tokens)
tokens = list(text)

print("Character Tokens:", tokens)

from transformers import AutoTokenizer

# Load tokenizer (BERT tokenizer uses WordPiece subword tokenization)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Input text
text = "Every moment is a beginning!"

# Tokenize text into subwords
tokens = tokenizer.tokenize(text)
print("Subword Tokens:", tokens)

# Convert tokens to token IDs
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Token IDs:", token_ids)

# Decode token IDs back to text
decoded_text = tokenizer.decode(token_ids)
print("Decoded Text:", decoded_text)

import nltk
nltk.download('punkt')  # Download tokenizer models
nltk.download('punkt_tab') # Download punkt_tab for better sentence tokenization

from nltk.tokenize import sent_tokenize, word_tokenize

# Updated sample text
text = "Artificial intelligence is most useful platform."

# Sentence tokenization
sentences = sent_tokenize(text)
print("Sentences:", sentences)

# Word tokenization for each sentence
words_per_sentence = [word_tokenize(sentence) for sentence in sentences]
print("Words per sentence:", words_per_sentence)

from transformers import AutoTokenizer

# Load the pre-trained tokenizer (BERT in this case)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Sample text
text = "Artificial intelligence is most running platform."

# Tokenize the text (subword tokenization)
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# Convert tokens to token IDs
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Token IDs:", token_ids)

# Decode token IDs back to text
decoded_text = tokenizer.decode(token_ids)
print("Decoded Text:", decoded_text)

from transformers import BertModel, BertTokenizer
import torch

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Example text
text = "Natural language processing is amazing."

# Tokenize the text
inputs = tokenizer(text, return_tensors='pt')

# Get model output
outputs = model(**inputs)

# Get embeddings (last hidden state)
embeddings = outputs.last_hidden_state

print(embeddings.shape)

from sklearn.preprocessing import OneHotEncoder  # Import OneHotEncoder from scikit-learn
import numpy as np  # Import NumPy for array handling

# Example categorical data
data = np.array(["cat", "dog", "bird", "cat"]).reshape(-1, 1)
# Reshape to a 2D array as required by OneHotEncoder (shape: [n_samples, 1])

# Initialize OneHotEncoder
encoder = OneHotEncoder(sparse_output=False)
# sparse_output=False returns a dense array instead of a sparse matrix

# Fit the encoder to the data and transform it into one-hot encoded vectors
one_hot = encoder.fit_transform(data)

# Print the resulting one-hot encoded array
print(one_hot)