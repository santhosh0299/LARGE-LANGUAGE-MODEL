# -*- coding: utf-8 -*-
"""Chatbot LLM 6th.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eKTnswD4o37cmY8WcwxPR0FolA_mNcMK
"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

model.eval()

print("ðŸ¤– GPT-2 Chatbot (type 'exit' to stop)\n")

chat_history = ""

while True:
    user_input = input("You: ")

    if user_input.lower() == "exit":
        print("Bot: Goodbye! ðŸ‘‹")
        break

    # Keep conversation limited (avoid very long repetition)
    chat_history += f"User: {user_input}\nBot:"

    input_ids = tokenizer.encode(chat_history, return_tensors="pt")

    # Limit input length (GPT-2 max is 1024)
    input_ids = input_ids[:, -800:]

    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_new_tokens=80,
            do_sample=True,
            top_k=50,
            top_p=0.9,
            temperature=0.7,
            repetition_penalty=1.2,
            no_repeat_ngram_size=3,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # Extract only new generated tokens
    new_tokens = output[0][input_ids.shape[-1]:]
    response = tokenizer.decode(new_tokens, skip_special_tokens=True)

    # Clean unwanted repeated lines
    response = response.split("User:")[0].strip()

    print("Bot:", response)

    chat_history += response + "\n"