# -*- coding: utf-8 -*-
"""Single head 2nd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dQoLhgdD4GXSv1mYsCdD_VrVbkJTsXoz
"""

import torch
import torch.nn.functional as F
import math

# batch_size = 1, sequence_length = 3, embedding_dimension = 4
X = torch.tensor([
    [[2.0, 1.0, 0.0, 1.0],
     [1.0, 0.0, 2.0, 1.0],
     [0.5, 1.5, 0.5, 2.0]]
])

# Changed weight values (manually set instead of random)
W_Q = torch.tensor([
    [0.2, 0.8, 0.5, 0.1],
    [0.9, 0.3, 0.4, 0.7],
    [0.6, 0.2, 0.9, 0.3],
    [0.1, 0.5, 0.2, 0.8]
])

W_K = torch.tensor([
    [0.3, 0.7, 0.6, 0.2],
    [0.8, 0.1, 0.5, 0.9],
    [0.4, 0.6, 0.3, 0.5],
    [0.2, 0.9, 0.7, 0.4]
])

W_V = torch.tensor([
    [0.5, 0.2, 0.8, 0.3],
    [0.7, 0.6, 0.1, 0.9],
    [0.2, 0.4, 0.5, 0.6],
    [0.9, 0.3, 0.7, 0.2]
])

# Query, Key, Value
Q = X @ W_Q
K = X @ W_K
V = X @ W_V

# QKᵀ / √d_k
attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.size(-1))

# Softmax
attention_weights = F.softmax(attention_scores, dim=-1)

# Final output
context_vector = torch.matmul(attention_weights, V)

print("Input X:\n", X)
print("\nQuery (Q):\n", Q)
print("\nKey (K):\n", K)
print("\nValue (V):\n", V)
print("\nAttention Scores:\n", attention_scores)
print("\nAttention Weights:\n", attention_weights)
print("\nContext Vector (Final Output):\n", context_vector)